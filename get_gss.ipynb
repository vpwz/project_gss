{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1wG5tUBJZAeC",
      "metadata": {
        "id": "1wG5tUBJZAeC"
      },
      "source": [
        "## Getting the GSS Data\n",
        "\n",
        "Since the data files are about 40GB zipped, we can't store a compressed or uncompressed version on GitHub, and the entire dataset can't really be loaded into memory with Colab.\n",
        "\n",
        "One option is to use Rivana: Download the data, unzip it, and work on it in a persistent environment.\n",
        "\n",
        "The other option is to avoid opening the entire file at once, and instead work with chunks of the data. That's what this code does for you.\n",
        "\n",
        "On GitHub, the data are broken into three smaller files, saved in .parquet format. The code below will load these chunks into memory, one at a time, you can specify the variables you want in `var_list`, and the results will be saved in `selected_gss_data.csv`.\n",
        "\n",
        "You can add more cleaning instructions in between the lines where the data are loaded ( `df = pd.read_parquet(url)`) and the data are saved (`df.loc...`). It's probably easiest to use this code to get only the variables you want, and then clean that subset of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "898a40de-9f3d-4dfc-8878-cd3badfdbcec",
      "metadata": {
        "id": "898a40de-9f3d-4dfc-8878-cd3badfdbcec",
        "outputId": "836d6ec5-dbd2-49ee-d09c-33351bb403ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://github.com/DS3001/project_gss/raw/main/gss_chunk_1.parquet\n",
            "Empty DataFrame\n",
            "Columns: [year, id, wrkstat, hrs1, hrs2, evwork, occ, prestige, wrkslf, wrkgovt, commute, industry, occ80, prestg80, indus80, indus07, occonet, found, occ10, occindv, occstatus, occtag, prestg10, prestg105plus, indus10, indstatus, indtag, marital, martype, agewed, divorce, widowed, spwrksta, sphrs1, sphrs2, spevwork, cowrksta, cowrkslf, coevwork, cohrs1, cohrs2, spocc, sppres, spwrkslf, spind, spocc80, sppres80, spind80, spocc10, spoccindv, spoccstatus, spocctag, sppres10, sppres105plus, spind10, spindstatus, spindtag, coocc10, coind10, paocc16, papres16, pawrkslf, paind16, paocc80, papres80, paind80, paocc10, paoccindv, paoccstatus, paocctag, papres10, papres105plus, paind10, paindstatus, paindtag, maocc80, mapres80, mawrkslf, maind80, maocc10, maoccindv, maoccstatus, maocctag, mapres10, mapres105plus, maind10, maindstatus, maindtag, sibs, childs, age, agekdbrn, educ, paeduc, maeduc, speduc, coeduc, codeg, degree, padeg, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 6694 columns]\n",
            "https://github.com/DS3001/project_gss/raw/main/gss_chunk_2.parquet\n",
            "Empty DataFrame\n",
            "Columns: [year, id, wrkstat, hrs1, hrs2, evwork, occ, prestige, wrkslf, wrkgovt, commute, industry, occ80, prestg80, indus80, indus07, occonet, found, occ10, occindv, occstatus, occtag, prestg10, prestg105plus, indus10, indstatus, indtag, marital, martype, agewed, divorce, widowed, spwrksta, sphrs1, sphrs2, spevwork, cowrksta, cowrkslf, coevwork, cohrs1, cohrs2, spocc, sppres, spwrkslf, spind, spocc80, sppres80, spind80, spocc10, spoccindv, spoccstatus, spocctag, sppres10, sppres105plus, spind10, spindstatus, spindtag, coocc10, coind10, paocc16, papres16, pawrkslf, paind16, paocc80, papres80, paind80, paocc10, paoccindv, paoccstatus, paocctag, papres10, papres105plus, paind10, paindstatus, paindtag, maocc80, mapres80, mawrkslf, maind80, maocc10, maoccindv, maoccstatus, maocctag, mapres10, mapres105plus, maind10, maindstatus, maindtag, sibs, childs, age, agekdbrn, educ, paeduc, maeduc, speduc, coeduc, codeg, degree, padeg, ...]\n",
            "Index: []\n",
            "\n",
            "[0 rows x 6694 columns]\n",
            "https://github.com/DS3001/project_gss/raw/main/gss_chunk_3.parquet\n",
            "       year  id            wrkstat  hrs1  hrs2 evwork  occ  prestige  \\\n",
            "20586  2022   1  working full time  40.0   NaN    NaN  NaN       NaN   \n",
            "20588  2022   3  working full time  52.0   NaN    NaN  NaN       NaN   \n",
            "20590  2022   5              other   NaN   NaN    yes  NaN       NaN   \n",
            "20591  2022   6  working full time  50.0   NaN    NaN  NaN       NaN   \n",
            "20593  2022   8  working full time  40.0   NaN    NaN  NaN       NaN   \n",
            "\n",
            "             wrkslf wrkgovt  ...  agehef12 agehef13 agehef14  hompoph  \\\n",
            "20586  someone else     NaN  ...       NaN      NaN      NaN      1.0   \n",
            "20588  someone else     NaN  ...       NaN      NaN      NaN      3.0   \n",
            "20590  someone else     NaN  ...       NaN      NaN      NaN      3.0   \n",
            "20591  someone else     NaN  ...       NaN      NaN      NaN      2.0   \n",
            "20593  someone else     NaN  ...       NaN      NaN      NaN      2.0   \n",
            "\n",
            "      wtssps_nea  wtssnrps_nea  wtssps_next wtssnrps_next  wtsscomp wtsscompnr  \n",
            "20586        NaN           NaN     0.230979      0.267499  0.231095   0.300990  \n",
            "20588        NaN           NaN          NaN           NaN  1.014148   1.363834  \n",
            "20590   1.704444      3.219778     1.693235      2.121712  1.083594   1.452537  \n",
            "20591        NaN           NaN          NaN           NaN  1.196792   1.357217  \n",
            "20593        NaN           NaN          NaN           NaN  0.580154   0.609534  \n",
            "\n",
            "[5 rows x 6694 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#\n",
        "var_list = ['year','income16', 'conpress','conmedic','consci','confed','conjudge','conlegis','conarmy'] # List of variables you want to save\n",
        "output_filename = 'cleaned_selected_gss_data.csv' # Name of the file you want to save the data to\n",
        "#\n",
        "modes = ['w','a'] # Has write mode and append mode\n",
        "phase = 0 # Starts in write mode; after one iteration of loop, switches to append mode\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "for k in range(3): # for each chunk of the data\n",
        "    url = 'https://github.com/DS3001/project_gss/raw/main/gss_chunk_' + str(1+k) + '.parquet' # Create url to the chunk to be processed\n",
        "    print(url) # Check the url is correct\n",
        "    df = pd.read_parquet(url) # Download this chunk of data\n",
        "    df = df[df['year'] == 2022]\n",
        "    df = df.dropna(subset=['income16','conpress','conmedic','consci','confed','conjudge','conlegis','conarmy'])\n",
        "    print(df.head()) # Visually inspect the first few rows\n",
        "    df.loc[:,var_list].to_csv(output_filename, # specifies target file to save the chunk to\n",
        "                              mode=modes[phase], # control write versus append\n",
        "                              header=var_list, # variable names\n",
        "                              index=False) # no row index saved\n",
        "    \n",
        "    phase = 1 # Switch from write mode to append mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6699b612",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nan' '$170,000 or over' '$50,000 to $59,999' '$75,000 to $89,999'\n",
            " '$60,000 to $74,999' '$30,000 to $34,999' 'under $1,000'\n",
            " '$8,000 to $9,999' '$12,500 to $14,999' '$40,000 to $49,999'\n",
            " '$5,000 to $5,999' '$35,000 to $39,999' '$25,000 to $29,999'\n",
            " '$90,000 to $109,999' '$22,500 to $24,999' '$20,000 to $22,499'\n",
            " '$110,000 to $129,999' '$150,000 to $169,999' '$130,000 to $149,999'\n",
            " '$1,000 to $2,999' '$17,500 to $19,999' '$6,000 to $6,999'\n",
            " '$10,000 to $12,499' '$15,000 to $17,499' '$7,000 to $7,999'\n",
            " '$3,000 to $3,999' '$4,000 to $4,999']\n"
          ]
        }
      ],
      "source": [
        "unique_values = df['income16'].astype(str).unique()\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "17d898ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nan' 'only some' 'hardly any' 'a great deal']\n"
          ]
        }
      ],
      "source": [
        "unique_values = df['conpress'].astype(str).unique()\n",
        "print(unique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4dd400f5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nan' 'a great deal' 'only some' 'hardly any']\n"
          ]
        }
      ],
      "source": [
        "unique_values = df['conmedic'].astype(str).unique()\n",
        "print(unique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3a5293df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nan' 'a great deal' 'only some' 'hardly any']\n"
          ]
        }
      ],
      "source": [
        "unique_values = df['consci'].astype(str).unique()\n",
        "print(unique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "12d11a02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nan' 'only some' 'a great deal' 'hardly any']\n"
          ]
        }
      ],
      "source": [
        "unique_values = df['confed'].astype(str).unique()\n",
        "print(unique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "28d3dccf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nan' 'hardly any' 'only some' 'a great deal']\n"
          ]
        }
      ],
      "source": [
        "unique_values = df['conlegis'].astype(str).unique()\n",
        "print(unique_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['nan' 'a great deal' 'only some' 'hardly any']\n"
          ]
        }
      ],
      "source": [
        "unique_values = df['conarmy'].astype(str).unique()\n",
        "print(unique_values)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "0.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
